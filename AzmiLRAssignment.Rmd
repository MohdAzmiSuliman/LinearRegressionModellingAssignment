---
title: "Linear Regression Modelling Assignment"
author: "Mohd Azmi"
date: "27/10/2019"
output:
  rmdformats::readthedown:
    highlight: kate
resource_files:
- AzmiLRAssignment.html
---
  
# Pre-amble
  
## Problem Background
  
A team of researchers wanted to find relationship between weight and head circumference among newborn. The researchers also wanted to know is there any other factors that can be used to predict the head circumference.

In this dataset, 550 terms babies were sampled from a population. their weight, length and head circumference were recorded at one month of age. The babies also had their gender and parity recorded. (Parity is their birth order in the family). Mother's education level were also recorded.

*Dataset Source: [Medical Statistics: A Guide to Data Analysis and Critical Appraisal](https://www.wiley.com/en-us/Medical+Statistics%3A+A+Guide+to+Data+Analysis+and+Critical+Appraisal-p-9780470755204)*


## Loading Library

```{r Loading Library, message=FALSE}
library(foreign)
library(tidyverse)
library(corrplot)
library(knitr)
library(broom)
library(tidyr)
library(psych)
library(rmdformats)
library(car)
```


## Loading Dataset

```{r Loading Dataset, message=FALSE}
Data01 <- read.spss("weights.sav", use.value.label=T, to.data.frame=T)
```

# Descriptive Analysis and Data Exploration

In the first step, data exploration was done to get the overview of the data.

## Descriptive Analysis

Descriptive analysis was done to the dataset to get the overview of the data, including statistic for each variables.

```{r Descriptive Exploratory}
kable(summary(Data01[, 5:6]))
kable(describe(Data01))
```

Based on the descriptive analysis, we can conclude that

- there were 550 samples
- mean and standard deviation of each numerical data - which include weight, length and head circumference
- distribution (count) of each categorical data - which include the baby's gender, mother's education level and parity

## Data Exploration

For data exploration, numerical data can be plotted into histogram, which can then be use to assess (qualitatively / eyeball) whether the data is normally distributed.  

### Histogram

Histogram for *Weight*

```{r Explore Numerical - Weight, message=FALSE}
ggplot(Data01, aes(weight)) +
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Data01$weight),
                            sd = sd(Data01$weight)))
```

The weight distribution look normally distributed  

Histogram for *Length*

```{r Explore Numerical - Length, message=FALSE}
ggplot(Data01, aes(length)) + 
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Data01$length),
                            sd = sd(Data01$length)))
```

The length distribution look normally distributed

Histogram for *Head Circumference*

```{r Explore Numerical - Head Circumference, message=FALSE}
ggplot(Data01, aes(headc)) + 
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Data01$headc),
                            sd = sd(Data01$headc)))
```

The weight distribution look normally distributed  
  
### Correlation

Correlation between numerical data can be calculated using correlation matrix and visualized using correlogram.

- Correlation Matrix and Correlogram

```{r Correlation and Correllogram}
Data02 <- Data01 %>% select_if(is.numeric)
CorData02 <- cor(Data02, use = "complete.obs", method = "pearson")
kable(round(CorData02,2))
corrplot(CorData02, method = "pie")
```

Based on the correlation Matrix, we can conclude that

- weight and length had high correlation, with r = 0.71
- other combination had moderate correlation, with 0.3 < r < 0.7


# Modelling - Univariable

Simple linear regression will be done to assess the association of each variable with head circumference.

## Univariable Models

### ModelWeight

ModelWeight was done using these variables

- outcome: Head Circumference
- covariate: weight

```{r LM ModelWeight}
ModelWeight <- lm(headc ~ weight, data = Data01)
summary(ModelWeight)
kable(tidy(ModelWeight, conf.int = T))
```

The weight is significantly associated with head circumference, in which for each increase in 1 kg of weight, the head circumference increase by 1.41 cm.

### ModelLength

ModelLength was done using these variables

- outcome: Head Circumference
- covariate: length

```{r LM ModelLength}
ModelLength <- lm(headc ~ length, data = Data01)
summary(ModelLength)
kable(tidy(ModelLength, conf.int = T))
```

The length is significantly associated with head circumference, in which for each increase in 1cm of length, the head circumference increase by 0.34 cm.


### ModelGender

ModelGender was done using these variables

- outcome: Head Circumference
- covariate: gender

```{r LM ModelGender}
ModelGender <- lm(headc ~ gender, data = Data01)
summary(ModelGender)
kable(tidy(ModelGender, conf.int = T))
```

The gender was significantly associated with head circumference.

### ModelEducation

ModelEducation was done using these variables

- outcome: Head Circumference
- covariate: Mother's Education

```{r LM ModelEducation}
ModelEducation <- lm(headc ~ educatio, data = Data01)
summary(ModelEducation)
kable(tidy(ModelEducation, conf.int = T))
```

There was no significant association between mother's education level and head circumference.


### ModelParity

ModelParity was done using these variables

- outcome: Head Circumference
- covariate: parity

```{r LM ModelParity}
ModelParity <- lm(headc ~ parity, data = Data01)
summary(ModelParity)
kable(tidy(ModelParity, conf.int = T))
```

There was significant association between the newborn parity and head circumference.

## Univariable Summary

all simple linear regressions were significant, except for variable mother education.

```{r Simple Linear Regression Result}
SLRResult <- matrix(c("Weight", 1.51, "(1.26, 1.56)", "< 0.001", 0.39,
                      "Length", 0.34, "(0.30, 0.38)", "< 0.001", 0.36,
                      "GenderFemale", -0.90, "(-1.12, -0.69)", "<0.001", 0.11,
                      "Mother's Education Level - Secondary", 0.09, "(-0.24, 0.42)", 0.592, "< 0.01",
                      "Mother's Education Level - Tertiary", 0.02, "(-0.23, 0.28", 0.848, "",
                      "Parity - One Sibling", 0.30, "(0.03, 0.58)", 0.032, 0.02,
                      "Parity - Two Siblings", 0.42, "(0.10, 0.73", 0.010, "",
                      "Parity - Three Siblings", 0.38, "(-0.01, 0.77)", 0.059, ""),  ncol = 5, byrow = T)
colnames(SLRResult) <- c("Variables", "beta", "(95% CI)", "p-value", "R^2^")
kable(SLRResult)
```


# Multivariable

To explore which covariate is the predictor for newborn's head circumference, several multiple linear regression models are created.

1. Model with weight and length as predictors
2. Model with all predictors included (weight, length, newborn's gender, mother's education level and newborn's parity)
3. Model with newborn's gender, mother's education level and newborn's parity as predictors

## Model Exploration

### ModelWeightLength

ModelWeightLength was done using these variables

- Outcome: Head Circumference
- Covariate: Weight and Length

```{r LM ModelWeightLength}
ModelWeightLength <- lm(headc ~ weight + length, data = Data01)
summary(ModelWeightLength)
kable(tidy(ModelWeightLength, conf.int = T))
```

### ModelWtLtGenEduPar

ModelWtLtGenEduPar was done using these variables

- Outcome: Head Circumference
- Covariate: Weight, Length + gender + mother education + parity

```{r LM ModelWtLtGenEduPar}
ModelWtLtGenEduPar <- lm(headc ~ weight + length + gender + educatio + parity, data = Data01)
summary(ModelWtLtGenEduPar)
kable(tidy(ModelWtLtGenEduPar, conf.int = T))
```

### ModelWtLtGenPar

ModelWtLtGenPar was done using these variables

- Outcome: Head Circumference
- Covariate: Weight, Length + gender + parity

```{r LM ModelWtLtGenPar}
ModelWtLtGenPar <- lm(headc ~ weight + length + gender + parity, data = Data01)
summary(ModelWtLtGenPar)
kable(tidy(ModelWtLtGenPar, conf.int = T))
```

## Model Comparison

All three model, ModelWtLt, ModelWtLtGenEduPar and ModelWtLtGenPar were compared.

```{r Model Comparison}
anova(ModelWeightLength, ModelWtLtGenEduPar)
anova(ModelWeightLength, ModelWtLtGenPar)
anova(ModelWtLtGenEduPar, ModelWtLtGenPar)
```

For model comparison, between ModelWtLtGenEduPar and ModelWtLtGenPar, there is no significant different. Thus, ModelWtLtGenEduPar was not selected.   

Thus, to choose either ModelWeightLength or ModelWtLtGenPar, adjusted R<sup>2</sup> will be use as guidance.  

For ModelWeightLength, the adjusted R<sup>2</sup> is 0.4339, while ModelWtLtGenPar, adjusted R<sup>2</sup>is 0.4452. thus ModelWtLtGenPar is chose.  

## Handling Confounding

There is high correlation between variable weight and length.

```{r Confounder Exploration}
ggplot(Data01, aes(weight, length)) +
  geom_point() +
  stat_smooth(method = 'lm', col = 'green')
```


Thus need to decide whether to include either one or both variables in the model.

- ModelGenParWt
- ModelGenParLt
- ModelGenParWtLt

### ModelGenParWt

ModelGenParWt was done using these variables

- outcome: head circumference
- covariate: Gender + Parity + **Weight**

```{r ModelGenParWt}
ModelGenParWt <- lm(headc ~ weight + gender + parity, data = Data01)
summary(ModelGenParWt)
kable(tidy(ModelGenParWt))
```

### ModelGenParLt

ModelGenParLt was done using these variables

- outcome: head circumference
- covariate: Gender + Parity + **Length**

```{r ModelGenParLt}
ModelGenParLt <- lm(headc ~ length + gender + parity, data = Data01)
summary(ModelGenParLt)
kable(tidy(ModelGenParLt))
```

### ModelGenParWtLT

ModelGenParWtLt was done using these variables

- outcome: head circumference
- covariate: Gender + Parity + **Weight** + **Length**

```{r ModelGenParWtLt}
ModelGenParWtLt <- lm(headc ~ weight + length + gender + parity, data = Data01)
summary(ModelGenParWtLt)
kable(tidy(ModelGenParWtLt))
```

### Confounder Model Comparison

for Weight

```{r Collinearity Weight Comparison}
ModelGenParWt01 <- tidy(ModelGenParWt)
ModelGenParWtLt01 <-tidy(ModelGenParWtLt)
((ModelGenParWtLt01 [2,2] - ModelGenParWt01 [2,2])/ ModelGenParWt01 [2,2])*100
```

for Length

```{r Collinearity Length Comparison}
ModelGenParLt01 <- tidy(ModelGenParLt)
((ModelGenParWtLt01 [2,2] - ModelGenParLt01 [2,2])/ ModelGenParLt01 [2,2])*100
```

Since Length have bigger difference, weight will be removed from the model. In addition to that, adjusted R<sup>2</sup> for model with length is higher than model with weight variable.


### Alternative for multicollinearity

To find multicollinearity between variables, VIF can be calculated.

```{r VIF}
kable(vif(lm(headc ~ weight + length + gender + parity, data = Data01)))
```

Based on this result, it's seem that while weight and lenght has higher VIF value compared to other variables, but the VIF value for both variables were still less than 10.

## Interaction

Model to check for interaction is

Outcome: Head Circumference
Covariate: length, gender and parity

### Interaction term - Length and Gender

ModelGenParLt_LtGen is used to examine interaction term between **length** and **gender**.

- Outcome: Head Circumference
- Covariate: length, gender, parity, length*gender

```{r ModelGenParLt_LtGen}
ModelGenParLt_LtGen <- lm(headc ~ length + gender + parity + length:gender, data = Data01)
summary(ModelGenParLt_LtGen)
kable(tidy(ModelGenParLt_LtGen))
```

The interaction model was compared with model without interaction term.

```{r ANOVA Interaction Lenght Gender}
anova(ModelGenParLt_LtGen, ModelGenParLt)
```

interaction term between length and gender is not significant.

### Interaction term - Length and Parity

ModelGenParLt_LtPar is used to examine interaction term between **length** and **parity**.

- Outcome: Head Circumference
- Covariate: length, gender, parity, length*parity

```{r ModelGenParLt_LtPar}
ModelGenParLt_LtPar <- lm(headc ~ length + gender + parity + length:parity, data = Data01)
summary(ModelGenParLt_LtPar)
kable(tidy(ModelGenParLt_LtPar))
```

The interaction model was compared with model without interaction term.

```{r ANOVA Interaction Lenght Parity}
anova(ModelGenParLt_LtPar, ModelGenParLt)
```

interaction term between length and parity is not significant

### Interaction term - Gender and Parity

ModelGenParLt_GenPar is used to examine interaction term between **parity** and **gender**.

- Outcome: Head Circumference
- Covariate: length, gender, parity, gender*parity

```{r ModelGenParLt_GenPar}
ModelGenParLt_GenPar <- lm(headc ~ length + gender + parity + gender:parity, data = Data01)
summary(ModelGenParLt_GenPar)
kable(tidy(ModelGenParLt_GenPar))
```

The interaction model was compared with model without interaction term.

```{r ANOVA Interaction Parity Gender}
anova(ModelGenParLt_GenPar, ModelGenParLt)
```

interaction term between gender and parity is not significant.

## Model Assessment

Preliminary Final Model need to be assessed by checking the model assumption. 


### Preliminary Final Model

Preliminary Final Model include these variables

- Outcome: Head Circumference
- Covariate: length, gender, parity

```{r Prelim Final Model}
ModelPrelim <- lm(headc ~ length + gender + parity, data = Data01)
summary(ModelPrelim)
kable(tidy(ModelPrelim))
```

### Prediction Value

Using Preliminary Final Model, prediction value was first calculated, by predicting the head circumference using the observed data from predictors variables (length, gender, parity)

```{r Prediction Preliminary Final Model}
Pred.PrelimFinalModel <- augment(ModelPrelim)
kable(head(Pred.PrelimFinalModel))
```

### Normality Test for Residual

Histogram for Residual to look for distribution shape.

```{r Histogram Residual, message=FALSE}
ggplot(data = Pred.PrelimFinalModel, aes(x=.resid)) + 
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Pred.PrelimFinalModel$.resid),
                            sd = sd(Pred.PrelimFinalModel$.resid)))
```

Distribution of model residual look normally distributed.

Scatter plot between predicted and residual values.

```{r Scatter Plot Predicted vs Residual}
ggplot(data = Pred.PrelimFinalModel, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline (yintercept = 0) +
  geom_hline (yintercept = 2, linetype=2, color="blue") +
  geom_hline (yintercept = -2, linetype=2, color="blue")
```
Scatter plot seem linear and constant

Thus, these two assumption for linear regression is fulfilled

1. for each set of X<sub>i</sub> values, there is a sub-population of Y values which are normally distributed.
2. the variance of the sub-population of Y are all equal


# Outlier and Influence

Outlier and influence may affect the model. thus samples with high (or large) influence need to be removed, and the final model should be run again. Comparison between model with all samples and without influence can be done.

## Explore Influence

Measure influential in the model

1. cooks distance: value above 1 or above 4/n
2. standardized residual: values above 2 or lower than -2
```{r Leverage Value}
(2*4 +2)/550
```
3. leverage above (2k +2)/n = 0.0182

## Select Influence

Sample with influence is selected using filter function
```{r Dataset Influence}
influen.obs <- Pred.PrelimFinalModel %>% filter(.std.resid > 2 | .std.resid < -2 | .hat > 0.0182)
summary(influen.obs)
kable(head(influen.obs))
```

There are 69 samples in the original dataset which were outlier and have high leverage.

## Select Dataset without Influence Samples

Sample without influence is selected using filter function
```{r Dataset without Influence}
NonInfluenObs <- Pred.PrelimFinalModel %>% filter(.std.resid < 2 & .std.resid > -2 & .hat < 0.0182)
summary(NonInfluenObs)
kable(head(NonInfluenObs))
```

When influence samples were removed, there were 481 samples.  

  
the model is re-run using dataset without influence

```{r Model with Dataset without Influence}
FinalModelNoInfluential <- lm(ModelPrelim, data = NonInfluenObs)
summary(FinalModelNoInfluential)
kable(tidy(FinalModelNoInfluential))
```

## Comparing Final Model with Final Model without Influence

### Final Model with Original Dataset (Without Influence removed)

Final Model with Original Dataset (Without Influence removed)

```{r Final Model Original Dataset}
ModelFinal <- ModelPrelim
summary(ModelFinal)
kable(tidy(ModelFinal, conf.int = T))
```


Residual Histogram

```{r Prediction Final Model Original Dataset, message=FALSE}
Pred.FinalModelOriginal <- augment(ModelFinal)
kable(head(Pred.FinalModelOriginal))
ggplot(data = Pred.FinalModelOriginal, aes(x=.resid)) +  
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Pred.FinalModelOriginal$.resid),
                            sd = sd(Pred.FinalModelOriginal$.resid)))
```

Residual Histogram look normally distributed

Scatter plot between Predicted and Residual value

```{r Scatterplot Final Model Original Dataset}
ggplot(data = Pred.FinalModelOriginal, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline (yintercept = 0) +
  geom_hline (yintercept = 2, linetype=2, color="blue") +
  geom_hline (yintercept = -2, linetype=2, color="blue")
```
Scatter plot seem linear and constant  
  
Thus, these two assumption for linear regression is fulfilled

1. for each set of $X$<sub>i</sub> values, there is a sub-population of $Y$ values which are normally distributed.
2. the variance of the sub-population of $Y$ are all equal

### Final Model with Influence Removed

Final Model with Influence Removed

```{r Final Model Without Influence}
summary(FinalModelNoInfluential)
kable(tidy(FinalModelNoInfluential))
```

Residual Histogram

```{r Prediction Final Model Without Influence, message=F}
Pred.FinalModelNoInfluence <- augment(FinalModelNoInfluential)
kable(head(Pred.FinalModelNoInfluence))
ggplot(data = Pred.FinalModelNoInfluence, aes(x=.resid)) +  
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Pred.FinalModelNoInfluence$.resid),
                            sd = sd(Pred.FinalModelNoInfluence$.resid)))
```
Residual Histogram look normally distributed

Scatter plot between Predicted and Residual value

```{r Scatterplot Final Model Without Influence}
ggplot(data = Pred.FinalModelNoInfluence, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline (yintercept = 0) +
  geom_hline (yintercept = 2, linetype=2, color="blue") +
  geom_hline (yintercept = -2, linetype=2, color="blue")
```
Scatter plot seem linear and constant  
  
Thus, these two assumption for linear regression is fulfilled

1. for each set of $X$<sub>i</sub> values, there is a sub-population of $Y$ values which are normally distributed.
2. the variance of the sub-population of $Y$ are all equal

Qualitatively, diagnostic plot for Final Model without Influence look *leaner*. Otherwise adjusted R^2^ for Final Model without Influence was higher.

Thus the final model will based on new model without influence samples.

# Final Model & Interpretation

## Final Model

Model  

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3$

- $X_1$ - length
- $X_2$ - gender
- $X_3$ - parity

```{r Final Model}
summary(FinalModelNoInfluential)
kable(tidy(FinalModelNoInfluential, conf.int = T))
```


head circumference = 19.96 + 0.33(length) - 0.55(female = 1) + 0.26(One sibling = 1) + 0.44(2 siblings = 1) + 0.26(3 or more siblings = 1)

## Model Interpretation

Multiple Linear Regression Analysis show that length, gender and parity had singificant linear relationship to head circumference. The model explains 48.8% of variation of head circumference in the study sample.  

The relationship can be explained as below

- those with 1 cm length increment have increment of 0.33cm (95% CI = 0.29, 0.37) head circumference when adjusted to gender and parity
- When comparing female against male, female baby will have 0.55cm (95% CI = 0.38, 0.72) lower head circumference, when adjusted to length and parity
- when comparing one sibling to singleton baby, one sibling will have 0.26cm (95% CI = 0.07, 0.46)  higher head circumference, when adjusted to length and gender
- when comparing two sibling to singleton baby, two sibling will have 0.44cm (95% CI = 0.22, 0.66) higher head circumference, when adjusted to length and gender
- when comparing 3 or more sibling to singleton baby, 3 or more sibling will have 0.17cm (95% CI = -0.17, 0.69) higher head circumference, when adjusted to length and gender

Model assumptions were met. There were no interaction nor multicollinearity between independent variables.  

Low coefficient of determination, R^2^ is most likely due to underfitting. There might be unexplained variation, which can be exlained by other variables outside of the scope of this study. But for this research purpose, with available variables, the model stated above is the best model available.

# Prediction

## Prediction with New Dataset

### Create New Dataset

new data was created using these parameter

- length - 45cm, mean newborn's length, 65cm
- gender - both male and female
- parity - singleton, one sibling, 2 siblings and 3 or more siblings


```{r New Data}
DS_cons <- expand.grid(length = c(45, mean(NonInfluenObs$length), 65),
                        gender = c('Male', 'Female'),
                        parity = c('Singleton', 'One sibling','2 siblings', '3 or more siblings'))
kable(head(DS_cons))
kable(tail(DS_cons))
```

### Calculate Predicted Values with New Dataset

```{r Predicted with New Data}
DataSetPred.NewDS <- augment(FinalModelNoInfluential, newdata = DS_cons)
kable(head(DataSetPred.NewDS))
kable(tail(DataSetPred.NewDS))
```


