---
title: "Linear Regression Modelling Assignment"
author: "Mohd Azmi"
date: "22/10/2019"
output: 
  rmdformats::readthedown:
    highlight: kate
---
  
# Pre-amble
  
## Problem Background
  
A team of researchers wanted to find relationship between weight and head circumference among newborn. The researchers also wanted to know is there any other factors that can be used to predict the head circumference.

In this dataset, 550 terms babies were sampled from a population. their weight, lenght and head circumference were recorded at one month of age. The babies also had their gender and parity recorded. (Parity is their birth order in the family). Mother's education level were also recorded.

*Dataset Source: [Medical Statistics: A Guide to Data Analysis and Critical Appraisal](https://www.wiley.com/en-us/Medical+Statistics%3A+A+Guide+to+Data+Analysis+and+Critical+Appraisal-p-9780470755204)*


## Loading Library

```{r Loading Library, message=FALSE}
library(foreign)
library(tidyverse)
library(corrplot)
library(knitr)
library(broom)
library(tidyr)
library(psych)
library(rmdformats)
```


## Loading Dataset

```{r Loading Dataset, message=FALSE}
Data01 <- read.spss("weights.sav", use.value.label=T, to.data.frame=T)
```

# Data Exploration

In the first step, data exploration was done to get the overview of the data.

## Descriptive

Descriptive analysis was done to the dataset to get the overview of the data, including statistic for each variables.

```{r Descriptive Exploratory}
kable(summary(Data01[, 2:6]))
kable(describe(Data01))
```

Based on the descriptive analysis, we can conclude that

- there were 550 samples
- mean and standard deviation of each numerical data - which include weight, length and head circumference
- distribution (count) of each categorical data - which include the baby's gender, mother's education level and parity

## Data Exploration

For data exploration, numerical data can be plotted into histogram, which can then be use to assess (qualitatively / eyeball) whether the data is normally distributed.  

### Histogram

Histogram for *Weight*

```{r Explore Numerical - Weight, message=FALSE}
ggplot(Data01, aes(weight)) +
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Data01$weight),
                            sd = sd(Data01$weight)))
```

The weight distribution look normally distributed  

Histogram for *Length*

```{r Explore Numerical - Length, message=FALSE}
ggplot(Data01, aes(length)) + 
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Data01$length),
                            sd = sd(Data01$length)))
```

The length distribution look normally distributed

Histogram for *Head Circumference*

```{r Explore Numerical - Head Circumference, message=FALSE}
ggplot(Data01, aes(headc)) + 
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Data01$headc),
                            sd = sd(Data01$headc)))
```

The weight distribution look normally distributed  
  
### Correlation

Correlation between numerical data can be calculated using correlation matrix and visuallized using correllogram.

- Correlation Matrix and Correlogram

```{r Correlation and Correllogram}
Data02 <- Data01 %>% select_if(is.numeric)
CorData02 <- cor(Data02, use = "complete.obs", method = "pearson")
kable(round(CorData02,2))
corrplot(CorData02, method = "pie")
```

Based on the correlation Matrix, we can conclude that

- weight and lenght had high correlation, with r = 0.71
- other combination had moderate correlation, with 0.3 < r < 0.7


# Modelling - Univariable

Simple linear regression will be done to assess the association of each variable with head circumference.

## Univariable Models

### ModelWeight

ModelWeight was done using these variables

- outcome: Head Circumference
- covariate: weight

```{r LM ModelWeight}
ModelWeight <- lm(headc ~ weight, data = Data01)
summary(ModelWeight)
kable(tidy(ModelWeight, conf.int = T))
```

The weight is significantly associated with head circumference, in which for each incrase in 1 kg of weight, the head circumference increase by 1.41 cm.

### ModelLength

ModelLength was done using these variables

- outcome: Head Circumference
- covariate: length

```{r LM ModelLength}
ModelLength <- lm(headc ~ length, data = Data01)
summary(ModelLength)
kable(tidy(ModelLength, conf.int = T))
```

The length is significanttly associated with head circumference, in which for each increase in 1cm of lenght, the head cicrcumference increase by 0.34 cm.


### ModelGender

ModelGender was done using these variables

- outcome: Head Circumference
- covariate: gender

```{r LM ModelGender}
ModelGender <- lm(headc ~ gender, data = Data01)
summary(ModelGender)
kable(tidy(ModelGender, conf.int = T))
```

The gender was significantly associated with head circumference.

### ModelEducation

ModelEducation was done using these variables

- outcome: Head Circumference
- covariate: Mother's Education

```{r LM ModelEducation}
ModelEducation <- lm(headc ~ educatio, data = Data01)
summary(ModelEducation)
kable(tidy(ModelEducation, conf.int = T))
```

There was significant association between mother's education level and head circumference.


### ModelParity

ModelParity was done using these variables

- outcome: Head Circumference
- covariate: parity

```{r LM ModelParity}
ModelParity <- lm(headc ~ parity, data = Data01)
summary(ModelParity)
kable(tidy(ModelParity, conf.int = T))
```

There is significant association between the newborn parity and head circumference.

## Univariable Summary

all simple linear regressions were significant, except for variable mother education.

```{r Simple Linear Regression Result}
SLRResult <- matrix(c("Weight", 1.51, "(1.26, 1.56)", "< 0.001", 0.39,
                      "Length", 0.34, "(0.30, 0.38)", "< 0.001", 0.36,
                      "GenderFemale", -0.90, "(-1.12, -0.69)", "<0.001", 0.11,
                      "Mother's Education Level - Secondary", 0.09, "(-0.24, 0.42)", 0.592, "< 0.01",
                      "Mother's Education Level - Tertiary", 0.02, "(-0.23, 0.28", 0.848, "< 0.01",
                      "Parity - One Sibling", 0.30, "(0.03, 0.58)", 0.032, 0.02,
                      "Parity - Two Siblings", 0.42, "(0.10, 0.73", 0.010, 0.02,
                      "Parity - Three Siblings", 0.38, "(-0.01, 0.77)", 0.059, 0.02),  ncol = 5, byrow = T)
colnames(SLRResult) <- c("Variables", "beta", "(95% CI)", "p-value", "R^2^")
kable(SLRResult)
```


# Multivariable

To explore which covariate is the predictor for newborn's head circumference, several multiple linear regression models are created.

## Model Exploration

### ModelWeightLength

ModelWeightLength was done using these variables

- Outcome: Head Circumference
- Covariate: Weight and Length

```{r LM ModelWeightLength}
ModelWeightLength <- lm(headc ~ weight + length, data = Data01)
summary(ModelWeightLength)
kable(tidy(ModelWeightLength, conf.int = T))
```

### ModelWtLtGenEduPar

ModelWtLtGenEduPar was done using these variables

- Outcome: Head Circumference
- Covariate: Weight, Length + gender + mother education + parity

```{r LM ModelWtLtGenEduPar}
ModelWtLtGenEduPar <- lm(headc ~ weight + length + gender + educatio + parity, data = Data01)
summary(ModelWtLtGenEduPar)
kable(tidy(ModelWtLtGenEduPar, conf.int = T))
```

### ModelWtLtGenPar

ModelWtLtGenPar was done using these variables

- Outcome: Head Circumference
- Covariate: Weight, Length + gender + parity

```{r LM ModelWtLtGenPar}
ModelWtLtGenPar <- lm(headc ~ weight + length + gender + parity, data = Data01)
summary(ModelWtLtGenEduPar)
kable(tidy(ModelWtLtGenPar, conf.int = T))
```

## Model Comparison

All three model, ModelWtLt, ModelWtLtGenEduPar and ModelWtLtGenPar were compared.

```{r Model Comparison}
anova(ModelWeightLength, ModelWtLtGenEduPar)
anova(ModelWeightLength, ModelWtLtGenPar)
anova(ModelWtLtGenEduPar, ModelWtLtGenPar)
```

For model comparison, between ModelWtLtGenEduPar and ModelWtLtGenPar, there is no significant different. Thus, ModelWtLtGenEduPar was not selected.   

Thus, to choose either ModelWeightLength or ModelWtLtGenPar, adjusted R<sup>2</sup> will be use as guidance.  

For ModelWeightLength, the adjusted R<sup>2</sup> is 0.4339, while ModelWtLtGenPar, adjusted R<sup>2</sup>is 0.4452. thus ModelWtLtGenPar is choosed.  

## Handling Confounding

There is high correlation between variable weight and length.

```{r Confounder Exploration}
ggplot(Data01, aes(weight, length)) + geom_point() + stat_smooth(method = 'lm', col = 'green')
```

Thus need to decide whether to include either one or both variables in the model.

- ModelGenParWt
- ModelGenParLt
- ModelGenParWtLt

### ModelGenParWt

ModelGenParWt was done using these variables

- outcome: head circumference
- covariate: Gender + Parity + **Weight**

```{r ModelGenParWt}
ModelGenParWt <- lm(headc ~ weight + gender + parity, data = Data01)
summary(ModelGenParWt)
ModelGenParWt01 <- tidy(ModelGenParWt)
kable(ModelGenParWt01)
```

### ModelGenParLt

ModelGenParLt was done using these variables

- outcome: head circumference
- covariate: Gender + Parity + **Length**

```{r ModelGenParLt}
ModelGenParLt <- lm(headc ~ length + gender + parity, data = Data01)
summary(ModelGenParLt)
ModelGenParLt01 <- tidy(ModelGenParLt)
kable(ModelGenParLt01)
```

### ModelGenParWtLT

ModelGenParWtLt was done using these variables

- outcome: head circumference
- covariate: Gender + Parity + **Weight** + **Length**

```{r ModelGenParWtLt}
ModelGenParWtLt <- lm(headc ~ weight + length + gender + parity, data = Data01)
summary(ModelGenParWtLt)
ModelGenParWtLt01 <-tidy(ModelGenParWtLt)
kable(ModelGenParWtLt01)
```

### Confounder Model Comparison

for Weight

```{r}
((ModelGenParWtLt01 [2,2] - ModelGenParWt01 [2,2])/ ModelGenParWt01 [2,2])*100
```

for Length

```{r}
((ModelGenParWtLt01 [2,2] - ModelGenParLt01 [2,2])/ ModelGenParLt01 [2,2])*100
```

Since Length have bigger difference, weight will be removed from the model. In addition to that, adjusted R<sup>2</sup> for model with length is higher than model with weight variable.


## Interaction

Model to check for interaction is

Outcome: Head Circumference
Covariate: length, gender and parity

### Interaction term - Length and Gender

ModelGenParLt_LtGen is used to examine interaction term between **length** and **gender**.

- Outcome: Head Circumference
- Covariate: length, gender, parity, length*gender

```{r ModelGenParLt_LtGen}
ModelGenParLt_LtGen <- lm(headc ~ length + gender + parity + length:gender, data = Data01)
summary(ModelGenParLt_LtGen)
kable(tidy(ModelGenParLt))
kable(tidy(ModelGenParLt_LtGen))
```

Both models were compared.

```{r ANOVA Interaction Lenght Gender}
anova(ModelGenParLt_LtGen, ModelGenParLt)
```

interaction term between length and gender is not significant.

### Interaction term - Length and Parity

ModelGenParLt_LtPar is used to examine interaction term between **length** and **parity**.

- Outcome: Head Circumference
- Covariate: length, gender, parity, length*parity

```{r ModelGenParLt_LtPar}
ModelGenParLt_LtPar <- lm(headc ~ length + gender + parity + length:parity, data = Data01)
summary(ModelGenParLt_LtPar)
kable(tidy(ModelGenParLt))
kable(tidy(ModelGenParLt_LtPar))
```

Both models were compared.

```{r ANOVA Interaction Lenght Parity}
anova(ModelGenParLt_LtPar, ModelGenParLt)
```

interaction term between length and parity is not significan

### Interaction term - Gender and Parity

ModelGenParLt_GenPar is used to examine interaction term between **parity** and **gender**.

- Outcome: Head Circumference
- Covariate: length, gender, parity, gender*parity

```{r ModelGenParLt_GenPar}
ModelGenParLt_GenPar <- lm(headc ~ length + gender + parity + gender:parity, data = Data01)
summary(ModelGenParLt_GenPar)
kable(tidy(ModelGenParLt))
kable(tidy(ModelGenParLt_GenPar))
```

Both models were compared.

```{r ANOVA Interaction Parity Gender}
anova(ModelGenParLt_GenPar, ModelGenParLt)
```

interaction term between gender and parity is not significant.

## Model Assesment

Preliminary Final Model need to be assessed by checking the model assumption. 


### Preliminary Final Model

Preliminary Final Model include these variables

- Outcome: Head Circumference
- Covariate: length, gender, parity

```{r Prelim Final Model}
ModelPrelim <- lm(headc ~ length + gender + parity, data = Data01)
summary(ModelPrelim)
kable(tidy(ModelPrelim))
```

### Prediction Value

Using Preliminary Final Model, prediction value was first calculated, by predicting the head circumference using the observed data from predictors variables (length, gender, parity and interaction term length*parity)

```{r Prediction Preliminary Final Model}
Pred.PrelimFinalModel <- augment(ModelPrelim)
kable(head(Pred.PrelimFinalModel))
```

### Normality Test for Residual

Histogram for Residual to look for distribution shape.

```{r Histogram Residual, message=FALSE}
ggplot(data = Pred.PrelimFinalModel, aes(x=.resid)) + 
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Pred.PrelimFinalModel$.resid),
                            sd = sd(Pred.PrelimFinalModel$.resid)))
```

Distribution of model residual look normally distributed.

Scatter plot between predicted and residual values.

```{r Scatter Plot Predicted vs Residual}
ggplot(data = Pred.PrelimFinalModel, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline (yintercept = 0) +
  geom_hline (yintercept = 2, linetype=2, color="blue") +
  geom_hline (yintercept = -2, linetype=2, color="blue")
```
Scatterplot seem linear and constant

Thus, these two assumption for linear regression is fulfilled

1. for each set of $X$<sub>i</sub> values, there is a subpopulation of Y values which are normally distributed.
2. the variance of the sub-population of Y are all equal


# Prediction

## Prediction with New Dataset

### Create New Dataset

new data was created using these parameter

- length - 45cm, 50cm, 55cm, 60cm, 65cm
- gender - both male and female
- parity - singleton, one sibling, 2 siblings and 3 or more siblings


```{r New Data}
DS_cons <- expand.grid(length = c(45, 50, 55, 60, 65),
                        gender = c('Male', 'Female'),
                        parity = c('Singleton', 'One sibling','2 siblings', '3 or more siblings'))
kable(head(DS_cons))
```

### Calculate Predicted Values with New Dataset

```{r Predicted with New Data}
DataSetPred.NewDS <- augment(ModelPrelim, newdata = DS_cons)
kable(head(DataSetPred.NewDS))
kable(tail(DataSetPred.NewDS))
```

# Outlier and Influence

Outlier and influence may affect the model. thus samples with high (or large influence) need to be removed, and the final model should be run again. Comparison between model with all samples and without influence can be done.

## Explore Influence

Measure influential in the model

1. cooks distance: value above 1 or above 4/n
2. standardized residual: values above 2 or lower than -2
```{r Leverage Value}
(2*4 +2)/550
```
3. leverage above (2k +2)/n = 0.0182

## Select Influence

Sample with influnce is selected using filter function
```{r Dataset Influence}
influen.obs <- Pred.PrelimFinalModel %>% filter(.std.resid > 2 | .std.resid < -2 | .hat > 0.0182)
summary(influen.obs)
kable(head(influen.obs))
```

## Select Dataset without Influence Samples

Sample without influence is seleected using filter function
```{r Dataset without Influence}
NonInfluenObs <- Pred.PrelimFinalModel %>% filter(.std.resid < 2 & .std.resid > -2 & .hat < 0.0182)
summary(NonInfluenObs)
kable(head(NonInfluenObs))
```

the model is re-run using dataset without influence

```{r Model with Dataset without Influence}
FinalModelNoInfluential <- lm(ModelPrelim, data = NonInfluenObs)
summary(FinalModelNoInfluential)
kable(tidy(FinalModelNoInfluential))
```

## Comparing Final Model with Final Model without Influence

### Final Model with Original Dataset (Without Influence removed)

Final Model with Original Dataset (Without Influence removed)

```{r Final Model Original Dataset}
ModelFinal <- ModelPrelim
summary(ModelFinal)
kable(tidy(ModelFinal, conf.int = T))
```


Residual Histogram

```{r Prediction Final Model Original Dataset, message=FALSE}
Pred.FinalModelOriginal <- augment(ModelFinal)
kable(head(Pred.FinalModelOriginal))
ggplot(data = Pred.FinalModelOriginal, aes(x=.resid)) +  
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Pred.FinalModelOriginal$.resid),
                            sd = sd(Pred.FinalModelOriginal$.resid)))
```

Residual Histogram look normally distributed

Scatterplot between Predicted and Residual value

```{r Scatterplot Final Model Original Dataset}
ggplot(data = Pred.FinalModelOriginal, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline (yintercept = 0) +
  geom_hline (yintercept = 2, linetype=2, color="blue") +
  geom_hline (yintercept = -2, linetype=2, color="blue")
```
Scatterplot seem linear and constant  
  
Thus, these two assumption for linear regression is fulfilled

1. for each set of $X$<sub>i</sub> values, there is a subpopulation of Y values which are normally distributed.
2. the variance of the sub-population of Y are all equal

### Final Model with Influence Removed

Final Model with Influence Removed

```{r Final Model Without Influence}
summary(FinalModelNoInfluential)
kable(tidy(FinalModelNoInfluential))
```

Residual Histogram

```{r Prediction Final Model Without Influence, message=F}
Pred.FinalModelNoInfluence <- augment(FinalModelNoInfluential)
kable(head(Pred.FinalModelNoInfluence))
ggplot(data = Pred.FinalModelNoInfluence, aes(x=.resid)) +  
  geom_histogram(aes(y=..density..)) +
  stat_function(fun = dnorm, colour = "magenta",
                args = list(mean = mean(Pred.FinalModelNoInfluence$.resid),
                            sd = sd(Pred.FinalModelNoInfluence$.resid)))
```
Residual Histogram look normally distributed

Scatterplot between Predicted and Residual value

```{r Scatterplot Final Model Withour Influence}
ggplot(data = Pred.FinalModelNoInfluence, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_hline (yintercept = 0) +
  geom_hline (yintercept = 2, linetype=2, color="blue") +
  geom_hline (yintercept = -2, linetype=2, color="blue")
```
Scatterplot seem linear and constant  
  
Thus, these two assumption for linear regression is fulfilled

1. for each set of $X$<sub>i</sub> values, there is a subpopulation of Y values which are normally distributed.
2. the variance of the sub-population of Y are all equal

Qualitatively, diagnostic plot for Final Model withour Influence look *leaner*, but otherwise does not seem different much.

# Final Model

Model  

$y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3$

- $X_1$ - length
- $X_2$ - gender
- $X_3$ - parity

```{r Final Model}
kable(tidy(ModelFinal, conf.int = T))
```


head circumference = 21.01 + 0.31(length) - 0.53(female = 1) + 0.27(One sibling = 1) + 0.38(2 siblings = 1) + 0.17(3 or more siblings = 1)
